{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1nYNsH3KUlt"
   },
   "source": [
    "Appears to be going very slow in google colab. Might just try on my computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjffFtEBhtNN"
   },
   "source": [
    "Need to import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "icR5-jc1h2f-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn # may need to do more specific things\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import stats\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics function. This is from Logistic Regression program.\n",
    "def evaluate_model(y_true, y_pred, model_name, dataset_size, split_num, model_weights, model_biases):\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Dataset Size': dataset_size,\n",
    "        'Split': split_num,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'Confusion Matrix': confusion_matrix(y_true, y_pred),\n",
    "        'Weights': model_weights, # not strictly metrics but useful to save\n",
    "        'Biases': model_biases,\n",
    "    }\n",
    "\n",
    "    # Class-specific metrics for binary classification\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        metrics.update({\n",
    "            'Sensitivity (TPR)': tp / (tp + fn),\n",
    "            'Specificity (TNR)': tn / (tn + fp),\n",
    "            'Precision (Class 1)': precision_score(y_true, y_pred, pos_label=1),\n",
    "            'Recall (Class 1)': recall_score(y_true, y_pred, pos_label=1)\n",
    "        })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "ZLrSVCi1oWWC",
    "outputId": "58c63371-a529-4095-e744-380c58b84a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1: Non-diabetic vs (Pre-diabetic or Diabetic)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f511c28e13410e88edc5174961fab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current layer size: 10\n",
      "Took 414.52256441116333 seconds\n",
      "Current layer size: 15\n",
      "Took 486.2524480819702 seconds\n",
      "Current layer size: 25\n",
      "Took 1144.7593071460724 seconds\n",
      "Best layer size: 25\n",
      "Best alpha: {'alpha': 0.05093501458684488}\n",
      "Best f1: 0.8796547630831797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Keep track of f1s and params in single list. Each element is a tuple: f1, alpha, layer1, layer2\\nf1_list = []\\n# Generate random paramters ahead of time\\nalphas = stats.loguniform.rvs(10**-6,1,size=num_test_times)\\nlayer1s = stats.loguniform.rvs(10,1000,size=num_test_times)\\nlayer2s = stats.loguniform.rvs(10,1000,size=num_test_times)\\nfor i in tqdm(range(num_test_times)):\\n  curr_alpha = alphas[i]\\n  curr_layer1 = layer1s[i]\\n  curr_layer2 = layer2s[i]\\n  # Test the model and get the f1 score\\n  curr_model = MLPClassifier(hidden_layer_sizes = (curr_layer1,curr_layer2),alpha = curr_alpha, max_iter = max_iterations, early_stopping = True)\\n  curr_model.fit(X_train,y_train_model1)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing a single split: Healhty vs. (diabetes or pre-diabetes)\n",
    "\n",
    "# Setup\n",
    "folderName = \"pca_splits/pca_splits/\"\n",
    "\n",
    "train_file_name = folderName + f\"train_size_100000_split_1.csv\"\n",
    "test_file_name = folderName + f\"test_size_100000_split_1.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Make an array of results for storage\n",
    "results = []\n",
    "\n",
    "\n",
    "# Separate features and targets\n",
    "pca_columns = [col for col in train_df.columns if col.startswith('PC')]\n",
    "target_columns = ['Diabetes_0', 'Diabetes_1', 'Diabetes_2']\n",
    "\n",
    "X_train = train_df[pca_columns]\n",
    "X_test = test_df[pca_columns]\n",
    "\n",
    "# Model 1: Non-diabetic (0) vs (Pre-diabetic or Diabetic) (1 or 2)\n",
    "print(\"\\nTraining Model 1: Non-diabetic vs (Pre-diabetic or Diabetic)\")\n",
    "\n",
    "# Create binary targets\n",
    "y_train_model1 = np.where((train_df['Diabetes_1'] == 1) | (train_df['Diabetes_2'] == 1), 1, 0)\n",
    "y_test_model1 = np.where((test_df['Diabetes_1'] == 1) | (test_df['Diabetes_2'] == 1), 1, 0)\n",
    "\n",
    "# Using the following to help write code:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "# https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py\n",
    "\n",
    "# Need to to a random search over hyperparameters to see what the best model is for testing.\n",
    "# Hyperparameters I want to modify: alpha (L2 regularization): loguniform from E-6 to 1, hidden layer sizes: two layers each lognormal from 10 to 1000 neurons\n",
    "\n",
    "num_test_times = 3 # number of alphas to test\n",
    "max_iterations = 1000 # don't know what it needs to be, I'll increase to as large size as I need\n",
    "\n",
    "# Testing to see how long it takes to train a single model\n",
    "# 10 neurons, 10 neurons: 66 seconds\n",
    "# 15, 15: 126 seconds\n",
    "# 25, 25: 110 seconds\n",
    "# 50, 50: 73 seconds. It might be due to early stopping. \n",
    "# May want to use my version of cross-validation from HW 3. \n",
    "\n",
    "\"\"\"\n",
    "init_t = time()\n",
    "curr_model = MLPClassifier(hidden_layer_sizes = (15,15),alpha = 10**-3, max_iter = max_iterations, early_stopping = True)\n",
    "curr_model.fit(X_train,y_train_model1)\n",
    "final_t = time()\n",
    "print(final_t - init_t,\"seconds have passed\")\n",
    "\"\"\"\n",
    "\n",
    "# I was having trouble using randomsearchcv for hidden_layer_sizes\n",
    "# so I'll use a combination of random search with alpha and manually\n",
    "# implemented grid search for the sizes of the layers.\n",
    "\n",
    "layersizes = [int(10**1.0), int(10**1.2), int(10**1.4)] # log uniform\n",
    "best_layer_size = int(10**1.0)\n",
    "best_f1 = 0\n",
    "best_param = None\n",
    "for layersize in tqdm(layersizes):\n",
    "    print(\"Current layer size:\",layersize)\n",
    "    init_t = time()\n",
    "    hyper_params = {\n",
    "        \"alpha\": stats.loguniform.rvs(10**-6,1,size=num_test_times),\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(MLPClassifier(hidden_layer_sizes = (layersize,layersize), max_iter = max_iterations, early_stopping = True),hyper_params,n_iter=num_test_times, scoring = 'f1',cv = 3)\n",
    "    random_search.fit(X_train,y_train_model1)\n",
    "    curr_params = random_search.best_params_\n",
    "    curr_f1 = random_search.best_score_\n",
    "    if curr_f1 > best_f1:\n",
    "        best_f1 = curr_f1\n",
    "        best_param = curr_params\n",
    "        best_layer_size = layersize\n",
    "    final_t = time()\n",
    "    print(\"Took\", final_t - init_t, \"seconds\")\n",
    "print(\"Best layer size:\", best_layer_size)\n",
    "print(\"Best alpha:\",best_param)\n",
    "print(\"Best f1:\",best_f1)\n",
    "\n",
    "# Get best model and predictions\n",
    "best_model1 = MLPClassifier(hidden_layer_sizes = (best_layer_size,best_layer_size), alpha=best_param['alpha'], max_iter = max_iterations, early_stopping = True)\n",
    "best_model1.fit(X_train,y_train_model1)\n",
    "y_pred_model1 = best_model1.predict(X_test)\n",
    "\n",
    "# See how well model does\n",
    "model1_metrics = evaluate_model(y_test_model1, y_pred_model1,\"MLP_Model1\", 10**5, 1, best_model1.coefs_, best_model1.intercepts_)\n",
    "results.append(model1_metrics)\n",
    "\n",
    "\"\"\"\n",
    "# I'm going to use RandomizedSearchCV to do cross-validation\n",
    "# Based on documentation and this tutorial: https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\n",
    "hyper_params = {\n",
    "    \"alpha\": stats.loguniform.rvs(10**-6,1,size=num_test_times),\n",
    "    #\"hidden_layer_sizes\": [stats.loguniform.rvs(10,1000,size=num_test_times),stats.loguniform.rvs(10,1000,size=num_test_times)]\n",
    "}\n",
    "random_search = RandomizedSearchCV(MLPClassifier(max_iter = max_iterations, early_stopping = True),hyper_params,n_iter=num_test_times)\n",
    "random_search.fit(X_train,y_train_model1)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Keep track of f1s and params in single list. Each element is a tuple: f1, alpha, layer1, layer2\n",
    "f1_list = []\n",
    "# Generate random paramters ahead of time\n",
    "alphas = stats.loguniform.rvs(10**-6,1,size=num_test_times)\n",
    "layer1s = stats.loguniform.rvs(10,1000,size=num_test_times)\n",
    "layer2s = stats.loguniform.rvs(10,1000,size=num_test_times)\n",
    "for i in tqdm(range(num_test_times)):\n",
    "  curr_alpha = alphas[i]\n",
    "  curr_layer1 = layer1s[i]\n",
    "  curr_layer2 = layer2s[i]\n",
    "  # Test the model and get the f1 score\n",
    "  curr_model = MLPClassifier(hidden_layer_sizes = (curr_layer1,curr_layer2),alpha = curr_alpha, max_iter = max_iterations, early_stopping = True)\n",
    "  curr_model.fit(X_train,y_train_model1)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "ZLrSVCi1oWWC",
    "outputId": "58c63371-a529-4095-e744-380c58b84a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 2: Pre-diabetic vs Diabetic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a999a774ce814fc9a8c85e549f10827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current layer size: 10\n",
      "Took 752.4629020690918 seconds\n",
      "Current layer size: 15\n",
      "Took 1165.4849123954773 seconds\n",
      "Current layer size: 25\n",
      "Took 1577.3784255981445 seconds\n",
      "Best layer size: 25\n",
      "Best alpha: {'alpha': 7.721530999421516e-06}\n",
      "Best f1: 0.7905554861099682\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [134546, 201819]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m y_pred_model2 \u001b[38;5;241m=\u001b[39m best_model2\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# See how well model does\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m model2_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_model2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_model2\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMLP_Model2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercepts_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(model2_metrics)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m# I'm going to use RandomizedSearchCV to do cross-validation\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m# Based on documentation and this tutorial: https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03mrandom_search.fit(X_train,y_train_model1)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(y_true, y_pred, model_name, dataset_size, split_num, model_weights, model_biases)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(y_true, y_pred, model_name, dataset_size, split_num, model_weights, model_biases):\n\u001b[0;32m      3\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset Size\u001b[39m\u001b[38;5;124m'\u001b[39m: dataset_size,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m'\u001b[39m: split_num,\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion Matrix\u001b[39m\u001b[38;5;124m'\u001b[39m: confusion_matrix(y_true, y_pred),\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeights\u001b[39m\u001b[38;5;124m'\u001b[39m: model_weights, \u001b[38;5;66;03m# not strictly metrics but useful to save\u001b[39;00m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBiases\u001b[39m\u001b[38;5;124m'\u001b[39m: model_biases,\n\u001b[0;32m     14\u001b[0m     }\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Class-specific metrics for binary classification\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs685hw\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs685hw\\lib\\site-packages\\sklearn\\metrics\\_classification.py:227\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m    226\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m--> 227\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs685hw\\lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cs685hw\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [134546, 201819]"
     ]
    }
   ],
   "source": [
    "# Testing a single split, but 2nd task\n",
    "\n",
    "folderName = \"pca_splits/pca_splits/\"\n",
    "\n",
    "train_file_name = folderName + f\"train_size_100000_split_1.csv\"\n",
    "test_file_name = folderName + f\"test_size_100000_split_1.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file_name)\n",
    "test_df = pd.read_csv(test_file_name)\n",
    "\n",
    "# Make an array of results for storage\n",
    "results = []\n",
    "\n",
    "\n",
    "# Separate features and targets\n",
    "pca_columns = [col for col in train_df.columns if col.startswith('PC')]\n",
    "target_columns = ['Diabetes_0', 'Diabetes_1', 'Diabetes_2']\n",
    "\n",
    "X_train = train_df[pca_columns]\n",
    "X_test = test_df[pca_columns]\n",
    "\n",
    "# Model 2: Pre-diabetic (1) vs Diabetic (1 or 2)\n",
    "print(\"\\nTraining Model 2: Pre-diabetic vs Diabetic\")\n",
    "\n",
    "# Filter samples that are either pre-diabetic or diabetic\n",
    "train_mask = (train_df['Diabetes_1'] == 1) | (train_df['Diabetes_2'] == 1)\n",
    "test_mask = (test_df['Diabetes_1'] == 1) | (test_df['Diabetes_2'] == 1)\n",
    "\n",
    "X_train_model2 = X_train[train_mask]\n",
    "X_test_model2 = X_test[test_mask]\n",
    "\n",
    "# Create binary targets (1 for pre-diabetic, 0 for diabetic)\n",
    "y_train_model2 = np.where(train_df.loc[train_mask, 'Diabetes_1'] == 1, 1, 0)\n",
    "y_test_model2 = np.where(test_df.loc[test_mask, 'Diabetes_1'] == 1, 1, 0)\n",
    "\n",
    "# Skip if not enough samples\n",
    "if len(np.unique(y_train_model2)) < 2 or len(np.unique(y_test_model2)) < 2:\n",
    "    print(f\"Skipping Model 2 for size 100,000, split 1 - not enough samples\")\n",
    "\n",
    "# Using the following to help write code:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "# https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py\n",
    "\n",
    "# Need to to a random search over hyperparameters to see what the best model is for testing.\n",
    "# Hyperparameters I want to modify: alpha (L2 regularization): loguniform from E-6 to 1, hidden layer sizes: two layers each lognormal from 10 to 1000 neurons\n",
    "\n",
    "num_test_times = 3 # number of alphas to test\n",
    "max_iterations = 1000 # don't know what it needs to be, I'll increase to as large size as I need\n",
    "\n",
    "# Testing to see how long it takes to train a single model\n",
    "# 10 neurons, 10 neurons: 66 seconds\n",
    "# 15, 15: 126 seconds\n",
    "# 25, 25: 110 seconds\n",
    "# 50, 50: 73 seconds. It might be due to early stopping. \n",
    "# May want to use my version of cross-validation from HW 3. \n",
    "\n",
    "\"\"\"\n",
    "init_t = time()\n",
    "curr_model = MLPClassifier(hidden_layer_sizes = (15,15),alpha = 10**-3, max_iter = max_iterations, early_stopping = True)\n",
    "curr_model.fit(X_train,y_train_model1)\n",
    "final_t = time()\n",
    "print(final_t - init_t,\"seconds have passed\")\n",
    "\"\"\"\n",
    "\n",
    "# I was having trouble using randomsearchcv for hidden_layer_sizes\n",
    "# so I'll use a combination of random search with alpha and manually\n",
    "# implemented grid search for the sizes of the layers.\n",
    "\n",
    "layersizes = [int(10**1.0), int(10**1.2), int(10**1.4)] # log uniform\n",
    "best_layer_size = int(10**1.0)\n",
    "best_f1 = 0\n",
    "best_param = None\n",
    "for layersize in tqdm(layersizes):\n",
    "    print(\"Current layer size:\",layersize)\n",
    "    init_t = time()\n",
    "    hyper_params = {\n",
    "        \"alpha\": stats.loguniform.rvs(10**-6,1,size=num_test_times),\n",
    "    }\n",
    "    random_search = RandomizedSearchCV(MLPClassifier(hidden_layer_sizes = (layersize,layersize), max_iter = max_iterations, early_stopping = True),hyper_params,n_iter=num_test_times, scoring = 'f1',cv = 3)\n",
    "    random_search.fit(X_train_model2,y_train_model2)\n",
    "    curr_params = random_search.best_params_\n",
    "    curr_f1 = random_search.best_score_\n",
    "    if curr_f1 > best_f1:\n",
    "        best_f1 = curr_f1\n",
    "        best_param = curr_params\n",
    "        best_layer_size = layersize\n",
    "    final_t = time()\n",
    "    print(\"Took\", final_t - init_t, \"seconds\")\n",
    "print(\"Best layer size:\", best_layer_size)\n",
    "print(\"Best alpha:\",best_param)\n",
    "print(\"Best f1:\",best_f1)\n",
    "\n",
    "# Get best model and predictions\n",
    "best_model2 = MLPClassifier(hidden_layer_sizes = (best_layer_size,best_layer_size), alpha=best_param['alpha'], max_iter = max_iterations, early_stopping = True)\n",
    "best_model2.fit(X_train_model2,y_train_model2)\n",
    "y_pred_model2 = best_model2.predict(X_test)\n",
    "\n",
    "# See how well model does\n",
    "model2_metrics = evaluate_model(y_test_model2, y_pred_model2,\"MLP_Model2\", 10**5, 1, best_model2.coefs_, best_model2.intercepts_)\n",
    "results.append(model2_metrics)\n",
    "\n",
    "\"\"\"\n",
    "# I'm going to use RandomizedSearchCV to do cross-validation\n",
    "# Based on documentation and this tutorial: https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\n",
    "hyper_params = {\n",
    "    \"alpha\": stats.loguniform.rvs(10**-6,1,size=num_test_times),\n",
    "    #\"hidden_layer_sizes\": [stats.loguniform.rvs(10,1000,size=num_test_times),stats.loguniform.rvs(10,1000,size=num_test_times)]\n",
    "}\n",
    "random_search = RandomizedSearchCV(MLPClassifier(max_iter = max_iterations, early_stopping = True),hyper_params,n_iter=num_test_times)\n",
    "random_search.fit(X_train,y_train_model1)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Keep track of f1s and params in single list. Each element is a tuple: f1, alpha, layer1, layer2\n",
    "f1_list = []\n",
    "# Generate random paramters ahead of time\n",
    "alphas = stats.loguniform.rvs(10**-6,1,size=num_test_times)\n",
    "layer1s = stats.loguniform.rvs(10,1000,size=num_test_times)\n",
    "layer2s = stats.loguniform.rvs(10,1000,size=num_test_times)\n",
    "for i in tqdm(range(num_test_times)):\n",
    "  curr_alpha = alphas[i]\n",
    "  curr_layer1 = layer1s[i]\n",
    "  curr_layer2 = layer2s[i]\n",
    "  # Test the model and get the f1 score\n",
    "  curr_model = MLPClassifier(hidden_layer_sizes = (curr_layer1,curr_layer2),alpha = curr_alpha, max_iter = max_iterations, early_stopping = True)\n",
    "  curr_model.fit(X_train,y_train_model1)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redoing this portion to avoid redoing whole thing\n",
    "y_pred_model2 = best_model2.predict(X_test_model2)\n",
    "\n",
    "# See how well model does\n",
    "model2_metrics = evaluate_model(y_test_model2, y_pred_model2,\"MLP_Model2\", 10**5, 1, best_model2.coefs_, best_model2.intercepts_)\n",
    "results.append(model2_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(results))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "r1nYNsH3KUlt"
   },
   "source": [
    "Cross validation times, while only testing one alpha:\n",
    "Layer 10: 480s = 8 min\n",
    "Layer 25: 492s = 8.2 min\n",
    "Layer 63: 1870s = 31 min :(\n",
    "In future, I'll just do 10-25 layers validation (about 24 minutes, which increases to 32 minutes if adding testing). Decreasing to 3 folds, increasing to 3 alpha values tested will increase this to 57.6 minutes. Adding other model takes 115.2 minutes, other databases 9.6 hr :((( I'll just work on this one then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results\n",
      "\n",
      "Saved results to MLP_results_pre_vs_diabedes.csv\n",
      "\n",
      "Summary Statistics:\n",
      "                         Accuracy  Precision   Recall  F1 Score\n",
      "Model      Dataset Size                                        \n",
      "MLP_Model2 100000         0.77789   0.779224  0.77789  0.777624\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving results\")\n",
    "# Convert results to DataFrame and save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_file = \"MLP_results_pre_vs_diabedes.csv\"\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"\\nSaved results to {results_file}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(results_df.groupby(['Model', 'Dataset Size'])[['Accuracy', 'Precision', 'Recall', 'F1 Score']].mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
